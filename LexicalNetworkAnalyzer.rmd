# ---
# title: "Lexical Data Analysis"
# author: Leo Niehorster-Cook
# ---
# 
# As you may have guessed, this notebook is for analyzing lexical networks. Thus
# far, it's fit to analyze the networks generated by PhonotacticNetworkGenerator, 
# <!-- but hopefully with time it will become broadly suited for analyzing network  -->
# representations of linguistic items.
# 
```{r} 
# Import Libraries

library(magrittr)
library(igraph)
library(spreadr)
library(data.table)

```

Since we'll be focusing on the lexicon, "lex," let's define some functions that
specify it so we can save some time.

```{r}
# This function takes in a graph, a vertex, and a length (n), 
# and returns the n-hop neighborhood of that vertex. Defaults to 1
nhophood <- 
  function(v, h = 1) lexicon.g %>%  
    induced_subgraph(
      vids = unlist(
        neighborhood(
          graph = lexicon.g,
          nodes = v,
          order = h
        )
      )
    )
```

```{r}
# This is a general "inspect" function; I happen to like these layouts.
# inspect <- 
#   function(network) network %>% 
#   plot(
#     layout = layout_with_graphopt(
#       network,
#       charge = 100, mass = .01,max.sa.movement = 2,
#       spring.length = 100000,spring.constant = .01
#     )
#   )

inspect <- 
  function(network) network %>% 
  plot(
    layout = layout.davidson.harel(lexicon.g)
    )

```

```{r}
# This function combines the last two. I prefer a different layout when I am
# specifically inspecting n-hop neighborhoods.

inspect_hood <- function(v, h = 1) nhophood(v, h) %>% 
  plot(
    layout = layout_as_tree(
    graph = nhophood(v, h),
    root = v,
    circular = TRUE
  )
)
```

```{r}
# This function returns the clustering coefficient, C(x), given just the word
# Note that igraph defines C(x) differently than as in Watts & Strogatz (1998)

cc <- 
  function(v) transitivity(graph = lexicon.g,type = "local",vids =v)

```

```{r}
# Our first experiment will simply measure a activation after ten ticks

initial <- function(word)
  data.frame(node = word, activation = 10, stringsAsFactors = F)
```


```{r}
# tst <- spreadr(g, df, retention = .5, time = 10, decay = 0, suppress = 0, create_names = TRUE)

experiment1 <- function(word) 
  spreadr(
    network = lexicon.g,
    start_run = data.frame(node = word, activation = 20, stringsAsFactors = F),
    retention = .5,
    time = 5,
    decay = 0,
    suppress = 0,
    create_names = FALSE
  ) 
```

```{r}
add_cc <-
  function(dt,word) data.table(dt)[,clustering:=cc(word)]
```

```{r}
peel_activation <- function(input)
  input[2]
```

```{r}
results1 <- 
  function(word) word %>% 
  experiment1() %>% 
  subset(time == max(time)) %>%
  subset(node == word)


#%>% #, select = word 
  #peel_activation() %>% 
  #as.numeric()
```

```{r}
# library(igraph)
# library(spreadr)
# 
# #initialize igraph smallworld object
# g <- sample_smallworld(dim = 1, size = 10, nei = 2, p = .2, loops = FALSE, multiple = FALSE)
# 
# #dataframe specifies which node is activated and the activation
# df <- data.frame("node" = 5, "activation" = 10)
# 
# #plot the smallworld
# plot(g)
# 
# #pass the smallworld we've constructed to the spreadr algorithm, along with the dataframe specifing initial values
# tst <- spreadr(g, df, retention = .5, time = 10, decay = 0, suppress = 0, create_names = TRUE)
# 
# 
# #profit
# print(tst)
```


```{r}
data2 <- 'low_prob_words.csv'
thing.dt <- fread(file = data2)
for (i in thing.dt){
  print(results1(i))
  print(degree(lexicon.g, i))
}
```


```{r}
data3 <- 'high_prob_words.csv'
thing.dt <- fread(file = data3)
for (i in thing.dt){
  print(results1(i))
  print(degree(lexicon.g, i))
}
```